0
00:00:00,000 --> 00:00:00,500


1
00:00:00,500 --> 00:00:04,090
In the next few videos, we'll be using a data set published

2
00:00:04,090 --> 00:00:08,530
by the United States Centers for Medicare and Medicaid Services

3
00:00:08,530 --> 00:00:12,990
to practice creating CART models to predict health care cost.

4
00:00:12,990 --> 00:00:16,400
We unfortunately can't use the D2Hawkeye data

5
00:00:16,400 --> 00:00:18,460
due to privacy issues.

6
00:00:18,460 --> 00:00:20,700
The data set we'll be using instead,

7
00:00:20,700 --> 00:00:26,300
ClaimsData.csv, is structured to represent a sample of patients

8
00:00:26,300 --> 00:00:28,870
in the Medicare program, which provides

9
00:00:28,870 --> 00:00:33,080
health insurance to Americans aged 65 and older,

10
00:00:33,080 --> 00:00:35,610
as well as some younger people with certain medical

11
00:00:35,610 --> 00:00:37,070
conditions.

12
00:00:37,070 --> 00:00:40,130
To protect the privacy of patients represented

13
00:00:40,130 --> 00:00:43,760
in this publicly available data set, a number of steps

14
00:00:43,760 --> 00:00:46,440
are performed to anonymize the data.

15
00:00:46,440 --> 00:00:49,180
So we would need to retrain the models we develop

16
00:00:49,180 --> 00:00:52,060
in this lecture on de-anonymized data

17
00:00:52,060 --> 00:00:55,510
if we wanted to apply our models in the real world.

18
00:00:55,510 --> 00:00:58,580
Let's start by reading our data set into R

19
00:00:58,580 --> 00:01:01,010
and taking a look at its structure.

20
00:01:01,010 --> 00:01:05,129
We'll call our data set Claims, and we'll

21
00:01:05,129 --> 00:01:10,755
use the read.csv function to read in the data file

22
00:01:10,755 --> 00:01:11,380
ClaimsData.csv.

23
00:01:11,380 --> 00:01:17,590


24
00:01:17,590 --> 00:01:20,420
Make sure to navigate to the directory on your computer

25
00:01:20,420 --> 00:01:25,390
containing the file ClaimsData.csv first.

26
00:01:25,390 --> 00:01:28,310
Now let's take a look at the structure of our data frame

27
00:01:28,310 --> 00:01:29,870
using the str function.

28
00:01:29,870 --> 00:01:32,920


29
00:01:32,920 --> 00:01:36,710
The observations represent a 1% random sample

30
00:01:36,710 --> 00:01:39,410
of Medicare beneficiaries, limited

31
00:01:39,410 --> 00:01:43,360
to those still alive at the end of 2008.

32
00:01:43,360 --> 00:01:46,560
Our independent variables are from 2008,

33
00:01:46,560 --> 00:01:50,590
and we will be predicting cost in 2009.

34
00:01:50,590 --> 00:01:54,450
Our independent variables are the patient's age

35
00:01:54,450 --> 00:01:59,570
in years at the end of 2008, and then several binary variables

36
00:01:59,570 --> 00:02:01,680
indicating whether or not the patient had

37
00:02:01,680 --> 00:02:04,590
diagnosis codes for a particular disease

38
00:02:04,590 --> 00:02:12,020
or related disorder in 2008: alzheimers, arthritis, cancer,

39
00:02:12,020 --> 00:02:17,730
chronic obstructive pulmonary disease, or copd, depression,

40
00:02:17,730 --> 00:02:21,970
diabetes, heart.failure, ischemic heart disease,

41
00:02:21,970 --> 00:02:29,290
or ihd, kidney disease, osteoporosis, and stroke.

42
00:02:29,290 --> 00:02:32,940
Each of these variables will take value 1 if the patient had

43
00:02:32,940 --> 00:02:37,150
a diagnosis code for the particular disease and value 0

44
00:02:37,150 --> 00:02:38,950
otherwise.

45
00:02:38,950 --> 00:02:42,900
Reimbursement2008 is the total amount

46
00:02:42,900 --> 00:02:46,490
of Medicare reimbursements for this patient in 2008.

47
00:02:46,490 --> 00:02:49,550
And reimbursement2009 is the total value

48
00:02:49,550 --> 00:02:54,010
of all Medicare reimbursements for the patient in 2009.

49
00:02:54,010 --> 00:02:59,040
Bucket2008 is the cost bucket the patient fell into in 2008,

50
00:02:59,040 --> 00:03:01,600
and bucket2009 is the cost bucket

51
00:03:01,600 --> 00:03:04,680
the patient fell into in 2009.

52
00:03:04,680 --> 00:03:08,670
These cost buckets are defined using the thresholds determined

53
00:03:08,670 --> 00:03:10,500
by D2Hawkeye.

54
00:03:10,500 --> 00:03:13,090
So the first cost bucket contains patients

55
00:03:13,090 --> 00:03:17,100
with costs less than $3,000, the second cost bucket

56
00:03:17,100 --> 00:03:22,110
contains patients with costs between $3,000 and $8,000,

57
00:03:22,110 --> 00:03:23,850
and so on.

58
00:03:23,850 --> 00:03:27,880
We can verify that the number of patients in each cost bucket

59
00:03:27,880 --> 00:03:29,630
has the same structure as what we

60
00:03:29,630 --> 00:03:33,560
saw for D2Hawkeye by computing the percentage of patients

61
00:03:33,560 --> 00:03:35,400
in each cost bucket.

62
00:03:35,400 --> 00:03:44,160
So we'll create a table of the variable bucket2009

63
00:03:44,160 --> 00:03:49,730
and divide by the number of rows in Claims.

64
00:03:49,730 --> 00:03:51,800
This gives the percentage of patients

65
00:03:51,800 --> 00:03:54,180
in each of the cost buckets.

66
00:03:54,180 --> 00:03:58,100
The first cost bucket has almost 70% of the patients.

67
00:03:58,100 --> 00:04:01,990
The second cost bucket has about 20% of the patients.

68
00:04:01,990 --> 00:04:05,740
And the remaining 10% are split between the final three cost

69
00:04:05,740 --> 00:04:06,920
buckets.

70
00:04:06,920 --> 00:04:12,470
So the vast majority of patients in this data set have low cost.

71
00:04:12,470 --> 00:04:15,830
Our goal will be to predict the cost bucket the patient fell

72
00:04:15,830 --> 00:04:19,720
into in 2009 using a CART model.

73
00:04:19,720 --> 00:04:21,800
But before we build our model, we

74
00:04:21,800 --> 00:04:26,610
need to split our data into a training set and a testing set.

75
00:04:26,610 --> 00:04:32,230
So we'll load the package caTools,

76
00:04:32,230 --> 00:04:35,600
and then we'll set our random seed to 88

77
00:04:35,600 --> 00:04:38,260
so that we all get the same split.

78
00:04:38,260 --> 00:04:43,430
And we'll use the sample.split function,

79
00:04:43,430 --> 00:04:51,070
where our dependent variable is Claims$bucket2009,

80
00:04:51,070 --> 00:04:56,160
and we'll set our SplitRatio to be 0.6.

81
00:04:56,160 --> 00:05:00,170
So we'll put 60% of the data in the training set.

82
00:05:00,170 --> 00:05:04,930
We'll call our training set ClaimsTrain,

83
00:05:04,930 --> 00:05:10,660
and we'll take the observations of Claims

84
00:05:10,660 --> 00:05:16,100
for which spl is exactly equal to TRUE.

85
00:05:16,100 --> 00:05:20,540
And our testing set will be called ClaimsTest,

86
00:05:20,540 --> 00:05:27,790
where we'll take the observations of Claims

87
00:05:27,790 --> 00:05:31,645
for which spl is exactly equal to FALSE.

88
00:05:31,645 --> 00:05:34,410


89
00:05:34,410 --> 00:05:37,950
Now that our data set is ready, we'll see in the next video

90
00:05:37,950 --> 00:05:41,570
how a smart baseline method would perform.

