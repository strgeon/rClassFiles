0
00:00:00,000 --> 00:00:00,500


1
00:00:00,500 --> 00:00:03,970
In the previous video, we got a feel for how regression trees

2
00:00:03,970 --> 00:00:06,970
can do things linear regression cannot.

3
00:00:06,970 --> 00:00:09,295
But what really matters at the end of the day

4
00:00:09,295 --> 00:00:12,790
is whether it can predict things better than linear regression.

5
00:00:12,790 --> 00:00:15,421
And so let's try that right now.

6
00:00:15,421 --> 00:00:17,420
We're going to try to predict house prices using

7
00:00:17,420 --> 00:00:20,060
all the variables we have available to us.

8
00:00:20,060 --> 00:00:24,990
So we'll load the caTools library.

9
00:00:24,990 --> 00:00:29,170
That will help us do a split on the data.

10
00:00:29,170 --> 00:00:33,120
We'll set the seed so our results are reproducible.

11
00:00:33,120 --> 00:00:41,610
And we'll say our split will be on the Boston house prices

12
00:00:41,610 --> 00:00:47,620
and we'll split it 70% training, 30% test.

13
00:00:47,620 --> 00:00:52,742
So our training data is a subset of the boston data

14
00:00:52,742 --> 00:00:55,600
where the split is TRUE.

15
00:00:55,600 --> 00:01:00,470
And the testing data is the subset of the boston data

16
00:01:00,470 --> 00:01:03,370
where the split is FALSE.

17
00:01:03,370 --> 00:01:07,255
OK, first of all, let's make a linear regression model,

18
00:01:07,255 --> 00:01:08,840
nice and easy.

19
00:01:08,840 --> 00:01:13,500
It's a linear model and the variables

20
00:01:13,500 --> 00:01:24,872
are latitude, longitude, crime, zoning, industry, whether it's

21
00:01:24,872 --> 00:01:33,050
on the Charles River or not, air pollution, rooms, age,

22
00:01:33,050 --> 00:01:40,110
distance, another form of distance, tax rates,

23
00:01:40,110 --> 00:01:43,622
and the pupil-teacher ratio.

24
00:01:43,622 --> 00:01:47,770
The data is training data.

25
00:01:47,770 --> 00:01:53,540
OK, let's see what our linear regression looks like.

26
00:01:53,540 --> 00:01:57,040
So we see that the latitude and longitude are not

27
00:01:57,040 --> 00:01:59,610
significant for the linear regression, which is perhaps

28
00:01:59,610 --> 00:02:01,981
not surprising because linear regression didn't seem

29
00:02:01,981 --> 00:02:04,660
to be able to take advantage of them.

30
00:02:04,660 --> 00:02:07,450
Crime is very important.

31
00:02:07,450 --> 00:02:10,225
The residential zoning might be important.

32
00:02:10,225 --> 00:02:11,600
Whether it's on the Charles River

33
00:02:11,600 --> 00:02:14,830
or not is a useful factor.

34
00:02:14,830 --> 00:02:16,460
Air pollution does seem to matter--

35
00:02:16,460 --> 00:02:19,770
the coefficient is negative, as you'd expect.

36
00:02:19,770 --> 00:02:22,710
The average number of rooms is significant.

37
00:02:22,710 --> 00:02:25,140
The age is somewhat important.

38
00:02:25,140 --> 00:02:28,170
Distance to centers of employment (DIS),

39
00:02:28,170 --> 00:02:30,030
is very important.

40
00:02:30,030 --> 00:02:33,440
Distance to highways and tax is somewhat important,

41
00:02:33,440 --> 00:02:37,924
and the pupil-teacher ratio is also very significant.

42
00:02:37,924 --> 00:02:39,340
Some of these might be correlated,

43
00:02:39,340 --> 00:02:42,205
so we can't put too much stock in necessarily interpreting

44
00:02:42,205 --> 00:02:44,594
them directly, but it's interesting.

45
00:02:44,594 --> 00:02:51,130
The adjusted R squared is 0.65, which is pretty good.

46
00:02:51,130 --> 00:02:55,685
So because it's kind of hard to compare out

47
00:02:55,685 --> 00:02:58,342
of sample accuracy for regression,

48
00:02:58,342 --> 00:03:00,300
we need to think of how we're going to do that.

49
00:03:00,300 --> 00:03:04,125
With classification, we just say, this method got X% correct

50
00:03:04,125 --> 00:03:06,480
and this method got Y% correct.

51
00:03:06,480 --> 00:03:09,190
Well, since we're doing continuous variables,

52
00:03:09,190 --> 00:03:12,130
let's calculate the sum of squared error, which

53
00:03:12,130 --> 00:03:15,360
we discussed in the original linear regression video.

54
00:03:15,360 --> 00:03:20,441
So let's say the linear regression's predictions are

55
00:03:20,441 --> 00:03:29,760
predict(linreg, newdata=test) and the linear regression sum

56
00:03:29,760 --> 00:03:37,040
of squared errors is simply the sum of the predicted values

57
00:03:37,040 --> 00:03:41,846
versus the actual values squared.

58
00:03:41,846 --> 00:03:51,240
So let's see what that number is-- 3,037.008.

59
00:03:51,240 --> 00:03:54,270
OK, so you know what we're interested to see

60
00:03:54,270 --> 00:03:58,940
now is, can we beat this using regression trees?

61
00:03:58,940 --> 00:04:01,840
So let's build a tree.

62
00:04:01,840 --> 00:04:04,960
The tree -- rpart command again.

63
00:04:04,960 --> 00:04:07,690
Actually to save myself from typing it all up again,

64
00:04:07,690 --> 00:04:11,600
I'm going to go back to the regression command

65
00:04:11,600 --> 00:04:18,279
and just change "lm" to "rpart" and change

66
00:04:18,279 --> 00:04:21,000
"linreg" to "tree"-- much easier.

67
00:04:21,000 --> 00:04:23,210
All right.

68
00:04:23,210 --> 00:04:26,170
So we've built our tree-- let's have a look at it using

69
00:04:26,170 --> 00:04:31,865
the "prp" command from "rpart.plot."

70
00:04:31,865 --> 00:04:33,430
And here we go.

71
00:04:33,430 --> 00:04:38,820
So again, latitude and longitude aren't really important

72
00:04:38,820 --> 00:04:41,510
as far as the tree's concerned.

73
00:04:41,510 --> 00:04:44,420
The rooms are the most important split.

74
00:04:44,420 --> 00:04:47,482
Pollution appears in there twice, so it's, in some sense,

75
00:04:47,482 --> 00:04:49,065
nonlinear on the amount of pollution--

76
00:04:49,065 --> 00:04:51,070
if it's greater than a certain amount

77
00:04:51,070 --> 00:04:53,860
or less than a certain amount, it does different things.

78
00:04:53,860 --> 00:04:56,490
Crime is in there, age is in there.

79
00:04:56,490 --> 00:04:58,990
Room appears three times, actually-- sorry.

80
00:04:58,990 --> 00:05:00,520
That's interesting.

81
00:05:00,520 --> 00:05:04,080
So it's very nonlinear on the number of rooms.

82
00:05:04,080 --> 00:05:06,590
Things that were important for the linear regression that

83
00:05:06,590 --> 00:05:11,300
don't appear in ours include pupil-teacher ratio.

84
00:05:11,300 --> 00:05:14,060
The DIS variable doesn't appear in our regression tree at all,

85
00:05:14,060 --> 00:05:15,540
either.

86
00:05:15,540 --> 00:05:17,850
So they're definitely doing different things,

87
00:05:17,850 --> 00:05:20,540
but how do they compare?

88
00:05:20,540 --> 00:05:24,280
So we'll predict, again, from the tree.

89
00:05:24,280 --> 00:05:31,180
"tree.pred" is the prediction of the tree on the new data.

90
00:05:31,180 --> 00:05:35,440


91
00:05:35,440 --> 00:05:38,320
And the tree sum of squared errors

92
00:05:38,320 --> 00:05:42,950
is the sum of the tree's predictions

93
00:05:42,950 --> 00:05:48,330
versus what they really should be.

94
00:05:48,330 --> 00:05:54,580
And then the moment of truth-- 4,328.

95
00:05:54,580 --> 00:05:58,100
So, simply put, regression trees are not as good

96
00:05:58,100 --> 00:06:00,970
as linear regression for this problem.

97
00:06:00,970 --> 00:06:04,225
What this says to us, given what we saw with the latitude

98
00:06:04,225 --> 00:06:07,320
and longitude, is that latitude and longitude are nowhere near

99
00:06:07,320 --> 00:06:09,860
as useful for predicting, apparently,

100
00:06:09,860 --> 00:06:12,480
as these other variables are.

101
00:06:12,480 --> 00:06:14,582
That's just the way it goes, I guess.

102
00:06:14,582 --> 00:06:16,540
It's always nice when a new method does better,

103
00:06:16,540 --> 00:06:18,539
but there's no guarantee that's going to happen.

104
00:06:18,539 --> 00:06:21,250
We need a special structure to really be useful.

105
00:06:21,250 --> 00:06:24,680
Let's stop here with the R and go back to the slides

106
00:06:24,680 --> 00:06:27,545
and discuss how CP works and then we'll

107
00:06:27,545 --> 00:06:29,555
apply cross validation to our tree.

108
00:06:29,555 --> 00:06:32,850
And we'll see if maybe we can improve in our results.

