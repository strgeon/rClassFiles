0
00:00:00,000 --> 00:00:00,499


1
00:00:00,499 --> 00:00:03,800
In the previous video, we preprocessed our data,

2
00:00:03,800 --> 00:00:07,230
and we're now ready to extract the word frequencies to be

3
00:00:07,230 --> 00:00:10,080
used in our prediction problem.

4
00:00:10,080 --> 00:00:12,660
The tm package provides a function called

5
00:00:12,660 --> 00:00:16,390
DocumentTermMatrix that generates a matrix where

6
00:00:16,390 --> 00:00:20,460
the rows correspond to documents, in our case tweets,

7
00:00:20,460 --> 00:00:24,270
and the columns correspond to words in those tweets.

8
00:00:24,270 --> 00:00:26,470
The values in the matrix are the number

9
00:00:26,470 --> 00:00:30,190
of times that word appears in each document.

10
00:00:30,190 --> 00:00:32,340
Let's go ahead and generate this matrix

11
00:00:32,340 --> 00:00:35,140
and call it "frequencies."

12
00:00:35,140 --> 00:00:40,530
So we'll use the DocumentTermMatrix function

13
00:00:40,530 --> 00:00:45,490
calls on our corpus that we created in the previous video.

14
00:00:45,490 --> 00:00:50,040
Let's take a look at our matrix by typing frequencies.

15
00:00:50,040 --> 00:00:54,570
We can see that there are 3,289 terms

16
00:00:54,570 --> 00:01:00,240
or words in our matrix and 1,181 documents

17
00:01:00,240 --> 00:01:03,730
or tweets after preprocessing.

18
00:01:03,730 --> 00:01:06,230
Let's see what this matrix looks like using

19
00:01:06,230 --> 00:01:08,280
the inspect function.

20
00:01:08,280 --> 00:01:26,370
So type  inspect(frequencies[1000:1005, 505:515]).

21
00:01:26,370 --> 00:01:30,210
In this range we see that the word "cheer" appears

22
00:01:30,210 --> 00:01:34,210
in the tweet 1005, but "cheap" doesn't

23
00:01:34,210 --> 00:01:37,510
appear in any of these tweets.

24
00:01:37,510 --> 00:01:39,950
This data is what we call sparse.

25
00:01:39,950 --> 00:01:43,600
This means that there are many zeros in our matrix.

26
00:01:43,600 --> 00:01:46,350
We can look at what the most popular terms are,

27
00:01:46,350 --> 00:01:49,380
or words, with the function findFreqTerms.

28
00:01:49,380 --> 00:01:53,900


29
00:01:53,900 --> 00:01:58,530
We want to call this on our matrix frequencies,

30
00:01:58,530 --> 00:02:03,260
and then we want to give an argument lowFreq, which

31
00:02:03,260 --> 00:02:05,680
is equal to the minimum number of times

32
00:02:05,680 --> 00:02:08,180
a term must appear to be displayed.

33
00:02:08,180 --> 00:02:10,110
Let's type 20.

34
00:02:10,110 --> 00:02:13,430
We see here 56 different words.

35
00:02:13,430 --> 00:02:18,370
So out of the 3,289 words in our matrix,

36
00:02:18,370 --> 00:02:22,990
only 56 words appear at least 20 times in our tweets.

37
00:02:22,990 --> 00:02:25,950
This means that we probably have a lot of terms

38
00:02:25,950 --> 00:02:29,920
that will be pretty useless for our prediction model.

39
00:02:29,920 --> 00:02:33,010
The number of terms is an issue for two main reasons.

40
00:02:33,010 --> 00:02:35,180
One is computational.

41
00:02:35,180 --> 00:02:38,170
More terms means more independent variables,

42
00:02:38,170 --> 00:02:42,160
which usually means it takes longer to build our models.

43
00:02:42,160 --> 00:02:44,450
The other is in building models, as we mentioned

44
00:02:44,450 --> 00:02:48,550
before, the ratio of independent variables to observations

45
00:02:48,550 --> 00:02:52,050
will affect how good the model will generalize.

46
00:02:52,050 --> 00:02:55,670
So let's remove some terms that don't appear very often.

47
00:02:55,670 --> 00:02:59,262
We'll call the output sparse, and we'll use

48
00:02:59,262 --> 00:03:00,678
the removeSparseTerms(frequencies,

49
00:03:00,678 --> 00:03:15,670
0.98).

50
00:03:15,670 --> 00:03:18,390
The sparsity threshold works as follows.

51
00:03:18,390 --> 00:03:22,400
If we say 0.98, this means to only keep

52
00:03:22,400 --> 00:03:25,890
terms that appear in 2% or more of the tweets.

53
00:03:25,890 --> 00:03:29,920
If we say 0.99, that means to only keep

54
00:03:29,920 --> 00:03:33,010
terms that appear in 1% or more of the tweets.

55
00:03:33,010 --> 00:03:37,570
If we say 0.995, that means to only keep

56
00:03:37,570 --> 00:03:41,060
terms that appear in 0.5% or more of the tweets,

57
00:03:41,060 --> 00:03:42,890
about six or more tweets.

58
00:03:42,890 --> 00:03:46,840
We'll go ahead and use this sparsity threshold.

59
00:03:46,840 --> 00:03:49,860
If you type sparse, you can see that there's

60
00:03:49,860 --> 00:03:53,900
only 309 terms in our sparse matrix.

61
00:03:53,900 --> 00:04:01,920
This is only about 9% of the previous count of 3,289.

62
00:04:01,920 --> 00:04:05,860
Now let's convert the sparse matrix into a data frame

63
00:04:05,860 --> 00:04:08,660
that we'll be able to use for our predictive models.

64
00:04:08,660 --> 00:04:16,649
We'll call it tweetsSparse and use the as.data.frame function

65
00:04:16,649 --> 00:04:22,730
called on the as.matrix function called on our matrix sparse.

66
00:04:22,730 --> 00:04:27,610
This converts sparse to a data frame called tweetsSparse.

67
00:04:27,610 --> 00:04:31,260
Since R struggles with variable names that start with a number,

68
00:04:31,260 --> 00:04:34,950
and we probably have some words here that start with a number,

69
00:04:34,950 --> 00:04:37,570
let's run the make.names function to make sure

70
00:04:37,570 --> 00:04:40,560
all of our words are appropriate variable names.

71
00:04:40,560 --> 00:04:45,540
To do this type colnames and then in parentheses the name

72
00:04:45,540 --> 00:04:51,568
of our data frame, tweetsSparse equals

73
00:04:51,568 --> 00:04:53,776


74
00:04:53,776 --> 00:04:55,234
make.names(colnames(tweetsSparse)).

75
00:04:55,234 --> 00:04:58,910


76
00:04:58,910 --> 00:05:01,120
This will just convert our variable names

77
00:05:01,120 --> 00:05:03,300
to make sure they're all appropriate names

78
00:05:03,300 --> 00:05:05,840
before we build our predictive models.

79
00:05:05,840 --> 00:05:07,440
You should do this each time you've

80
00:05:07,440 --> 00:05:11,570
built a data frame using text analytics.

81
00:05:11,570 --> 00:05:14,751
Now let's add our dependent variable to this data set.

82
00:05:14,751 --> 00:05:16,292
We'll call it tweetsSparse$Negative

83
00:05:16,292 --> 00:05:16,958
and set it equal to the original Negative variable from the tweets data frame.

84
00:05:16,958 --> 00:05:27,090


85
00:05:27,090 --> 00:05:30,470
Lastly, let's split our data into a training set

86
00:05:30,470 --> 00:05:34,290
and a testing set, putting 70% of the data in the training

87
00:05:34,290 --> 00:05:35,400
set.

88
00:05:35,400 --> 00:05:39,480
First we'll have to load the library caTools so that we

89
00:05:39,480 --> 00:05:41,860
can use the sample.split function.

90
00:05:41,860 --> 00:05:47,870
Then let's set the seed to 123 and create our split using

91
00:05:47,870 --> 00:05:51,249
sample.split where our dependent variable is

92
00:05:51,249 --> 00:05:52,165
tweetsSparse$Negative.

93
00:05:52,165 --> 00:05:56,660


94
00:05:56,660 --> 00:06:00,810
And then our split ratio will be 0.7.

95
00:06:00,810 --> 00:06:05,150
We'll put 70% of the data in the training set.

96
00:06:05,150 --> 00:06:08,950
Then let's just use subset to create a treating set called

97
00:06:08,950 --> 00:06:14,320
trainSparse, which will take a subset of the whole data set

98
00:06:14,320 --> 00:06:17,770
tweetsSparse, but always take the observations for which

99
00:06:17,770 --> 00:06:20,030
split is equal to TRUE.

100
00:06:20,030 --> 00:06:23,280
And we'll create our test set, testSparse,

101
00:06:23,280 --> 00:06:27,780
again using subset to take the observations of tweetsSparse,

102
00:06:27,780 --> 00:06:29,430
but this time for which split is equal to FALSE.

103
00:06:29,430 --> 00:06:32,150


104
00:06:32,150 --> 00:06:35,890
Our data is now ready, and we can build our predictive model.

105
00:06:35,890 --> 00:06:39,670
In the next video, we'll use CART and logistic regression

106
00:06:39,670 --> 00:06:41,924
to predict negative sentiment.

107
00:06:41,924 --> 00:06:42,424


