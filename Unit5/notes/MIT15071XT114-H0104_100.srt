0
00:00:00,000 --> 00:00:00,499


1
00:00:00,499 --> 00:00:02,640
Now let's build the document-term matrix

2
00:00:02,640 --> 00:00:03,746
for our corpus.

3
00:00:03,746 --> 00:00:05,120
So we'll create a variable called

4
00:00:05,120 --> 00:00:07,275
dtm that contains the DocumentTermMatrix(corpus).

5
00:00:07,275 --> 00:00:11,510


6
00:00:11,510 --> 00:00:15,240
The corpus has already had all the pre-processing run on it.

7
00:00:15,240 --> 00:00:18,870
So to get the summary statistics about the document-term matrix,

8
00:00:18,870 --> 00:00:21,842
we'll just type in the name of our variable, dtm.

9
00:00:21,842 --> 00:00:24,275
And what we can see is that even though we

10
00:00:24,275 --> 00:00:27,860
have only 855 emails in the corpus,

11
00:00:27,860 --> 00:00:31,890
we have over 22,000 terms that showed up at least once,

12
00:00:31,890 --> 00:00:34,290
which is clearly too many variables

13
00:00:34,290 --> 00:00:36,550
for the number of observations we have.

14
00:00:36,550 --> 00:00:38,280
So we want to remove the terms that

15
00:00:38,280 --> 00:00:41,030
don't appear too often in our data set,

16
00:00:41,030 --> 00:00:46,597
and we'll do that using the removeSparseTerms function.

17
00:00:46,597 --> 00:00:48,680
And we're going to have to determine the sparsity,

18
00:00:48,680 --> 00:00:51,920
so we'll say that we'll remove any term that doesn't appear

19
00:00:51,920 --> 00:00:54,320
in at least 3% of the documents.

20
00:00:54,320 --> 00:01:00,720
To do that, we'll pass 0.97 to removeSparseTerms.

21
00:01:00,720 --> 00:01:03,190
Now we can take a look at the summary statistics

22
00:01:03,190 --> 00:01:05,271
for the document-term matrix, and we

23
00:01:05,271 --> 00:01:07,270
can see that we've decreased the number of terms

24
00:01:07,270 --> 00:01:11,380
to 788, which is a much more reasonable number.

25
00:01:11,380 --> 00:01:14,970
So let's build a data frame called labeledTerms out

26
00:01:14,970 --> 00:01:16,860
of this document-term matrix.

27
00:01:16,860 --> 00:01:20,326
So to do this, we'll use as.data.frame

28
00:01:20,326 --> 00:01:26,080
of as.matrix applied to dtm, the document-term matrix.

29
00:01:26,080 --> 00:01:29,330
So this data frame is only including right now

30
00:01:29,330 --> 00:01:32,380
the frequencies of the words that appeared in at least 3%

31
00:01:32,380 --> 00:01:36,050
of the documents, but in order to run our text analytics

32
00:01:36,050 --> 00:01:39,670
models, we're also going to have the outcome variable, which

33
00:01:39,670 --> 00:01:42,650
is whether or not each email was responsive.

34
00:01:42,650 --> 00:01:45,280
So we need to add in this outcome variable.

35
00:01:45,280 --> 00:01:49,745
So we'll create labeledTerms$responsive,

36
00:01:49,745 --> 00:01:52,770
and we'll simply copy over the responsive variable from

37
00:01:52,770 --> 00:01:55,240
the original emails data frame so it's equal

38
00:01:55,240 --> 00:01:56,375
to emails$responsive.

39
00:01:56,375 --> 00:02:00,480


40
00:02:00,480 --> 00:02:03,400
So finally let's take a look at our newly constructed data

41
00:02:03,400 --> 00:02:05,510
frame with the str function.

42
00:02:05,510 --> 00:02:08,580


43
00:02:08,580 --> 00:02:14,850
So as we expect, there are an awful lot of variables, 789 in total.

44
00:02:14,850 --> 00:02:17,630
788 of those variables are the frequencies

45
00:02:17,630 --> 00:02:21,860
of various words in the emails, and the last one is responsive,

46
00:02:21,860 --> 00:02:23,970
the outcome variable.

